{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for testing MONAI transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "path = os.path.dirname(current_directory)\n",
    "sys.path.append(path)\n",
    "from Utils import *\n",
    "\n",
    "%matplotlib widget\n",
    "from ipywidgets import interact, interactive, widgets\n",
    "from matplotlib.patches import Rectangle, Circle, Arrow\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Any, Mapping, Hashable\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.utils import first\n",
    "from monai.config import KeysCollection\n",
    "from monai.data import Dataset, ArrayDataset, create_test_image_3d, DataLoader\n",
    "from monai.transforms import (\n",
    "    AdjustContrastd,\n",
    "    Transform,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    Orientation,\n",
    "    ConcatItemsd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureChannelFirst,\n",
    "    ToTensord,\n",
    "    Spacingd,\n",
    "    ScaleIntensityd,\n",
    "    CropForegroundd,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param():\n",
    "    def __init__(self, data_dir, pixel_dim, window_size, orientation, in_channels, out_channels, input_type, label_type):\n",
    "        self.data_dir = data_dir\n",
    "        self.pixel_dim = pixel_dim\n",
    "        self.window_size = window_size\n",
    "        self.axcodes = orientation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_type = input_type\n",
    "        self.label_type = label_type\n",
    "\n",
    "def generateLabeledFileList(param, prefix):\n",
    "    print('Reading labeled images from: ' + param.data_dir)\n",
    "    images_m = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_M.nii.gz\")))\n",
    "    images_p = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_P.nii.gz\")))\n",
    "    images_r = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_R.nii.gz\")))\n",
    "    images_i = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_I.nii.gz\")))\n",
    "    labels = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_labels\", \"*_\"+param.label_type+\"_label.nii.gz\")))\n",
    "    \n",
    "    # Use two types of images combined\n",
    "    if param.in_channels==2:\n",
    "        # Use real and imaginary images\n",
    "        if param.input_type=='R' or param.input_type=='I':\n",
    "            print('Use real/imaginary images')\n",
    "            data_dicts = [\n",
    "                {\"image_1\": image_r_name, \"image_2\": image_i_name, \"label\":label_name}\n",
    "                for image_r_name, image_i_name, label_name in zip(images_r, images_i, labels)\n",
    "            ]\n",
    "        # Use magnitude and phase images\n",
    "        else:\n",
    "            print('Use magnitude/phase images')\n",
    "            data_dicts = [\n",
    "                {\"image_1\": image_m_name, \"image_2\": image_p_name, \"label\":label_name}\n",
    "                for image_m_name, image_p_name, label_name in zip(images_m, images_p, labels)\n",
    "            ]\n",
    "    # Use only one type of image        \n",
    "    else:\n",
    "        # Use real images\n",
    "        if param.input_type=='R':\n",
    "            print('Use real images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_r, labels)\n",
    "            ]\n",
    "        # Use imaginary images\n",
    "        elif param.input_type=='I':\n",
    "            print('Use imaginary images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_i, labels)\n",
    "            ]\n",
    "        # Use phase images\n",
    "        elif param.input_type=='P':\n",
    "            print('Use phase images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_p, labels)\n",
    "            ]\n",
    "        # Use magnitude images\n",
    "        else:\n",
    "            print('Use magnitude images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_m, labels)\n",
    "            ]\n",
    "    return data_dicts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dictionary\n",
      "Reading labeled images from: /Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook\n",
      "Use magnitude/phase images\n",
      "{'image_1': '/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/test_images/SyntheticImage_018_M.nii.gz', 'image_2': '/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/test_images/SyntheticImage_018_P.nii.gz', 'label': '/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/test_labels/SyntheticImage_018_multi_label.nii.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Build param (info from config.ini)\n",
    "data_dir = os.getcwd()\n",
    "pixel_dim = (3.6, 1.171875, 1.171875)\n",
    "window_size = (3, 48, 48)\n",
    "orientation = 'PIL'\n",
    "in_channels = 2\n",
    "out_channels = 3\n",
    "input_type = 'MP'\n",
    "label_type = 'multi'\n",
    "param = Param(data_dir, pixel_dim, window_size, orientation, in_channels, out_channels, input_type, label_type)\n",
    "\n",
    "# Create dictionary\n",
    "print('Create dictionary')\n",
    "train_files = generateLabeledFileList(param, 'test')\n",
    "print(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE NEW CUSTOM ITK OBJECT LOADER\n",
    "\n",
    "import itk\n",
    "from monai.data import ITKReader, MetaTensor\n",
    "import torch\n",
    "from monai.config import DtypeLike\n",
    "from monai.utils import convert_to_dst_type\n",
    "from monai.utils import ImageMetaKey as Key\n",
    "\n",
    "class LoadITKObject(Transform):\n",
    "    def __init__(self,\n",
    "            image_only: bool = False,\n",
    "            dtype: DtypeLike or None = np.float32,\n",
    "            ensure_channel_first: bool = False,\n",
    "            simple_keys: bool = False,\n",
    "            prune_meta_pattern: str or None = None,\n",
    "            prune_meta_sep: str = \".\",\n",
    "            expanduser: bool = True,         \n",
    "        ) -> None:\n",
    "        self.itkReader = ITKReader()\n",
    "        self.image_only = image_only\n",
    "        self.ensure_channel_first = ensure_channel_first\n",
    "        self.dtype = dtype\n",
    "        self.simple_keys = simple_keys\n",
    "        self.pattern = prune_meta_pattern\n",
    "        self.sep = prune_meta_sep\n",
    "        self.expanduser = expanduser\n",
    "\n",
    "    def __call__(self, img:itk.itkImagePython.itkImageF3):\n",
    "        if isinstance(img, itk.itkImagePython.itkImageF3) == 0:\n",
    "            raise RuntimeError(f\"{self.__class__.__name__} The input image is not an ITK object.\\n\")\n",
    "        img_array, meta_data = self.itkReader.get_data(img)\n",
    "        img_array = convert_to_dst_type(img_array, dst=img_array, dtype=self.dtype)[0]\n",
    "        if not isinstance(meta_data, dict):\n",
    "            raise ValueError(f\"`meta_data` must be a dict, got type {type(meta_data)}.\")\n",
    "        # Here I changed from original LoadImage to use tensor instead of numpy array (img_array) \n",
    "        # so the result is similar to loading the nifti file with LoadImage\n",
    "        img = MetaTensor.ensure_torch_and_prune_meta(\n",
    "            torch.from_numpy(img_array), meta_data, self.simple_keys, pattern=self.pattern, sep=self.sep\n",
    "        )\n",
    "        if self.ensure_channel_first:\n",
    "            img = EnsureChannelFirst()(img)\n",
    "        if self.image_only:\n",
    "            return img\n",
    "        return img, img.meta if isinstance(img, MetaTensor) else meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[[0.3549, 0.3380, 0.3352],\n",
      "         [0.3519, 0.3691, 0.3125],\n",
      "         [0.3634, 0.3661, 0.3520],\n",
      "         ...,\n",
      "         [0.2452, 0.2340, 0.1959],\n",
      "         [0.2745, 0.2437, 0.2493],\n",
      "         [0.3192, 0.2761, 0.2805]],\n",
      "\n",
      "        [[0.3495, 0.3437, 0.3300],\n",
      "         [0.3317, 0.3535, 0.3084],\n",
      "         [0.3516, 0.3400, 0.3551],\n",
      "         ...,\n",
      "         [0.2688, 0.2568, 0.1992],\n",
      "         [0.2424, 0.2224, 0.1931],\n",
      "         [0.2454, 0.2236, 0.2136]],\n",
      "\n",
      "        [[0.3308, 0.3461, 0.3214],\n",
      "         [0.3149, 0.3304, 0.3110],\n",
      "         [0.3439, 0.3199, 0.3542],\n",
      "         ...,\n",
      "         [0.3304, 0.3056, 0.2455],\n",
      "         [0.2756, 0.2674, 0.2069],\n",
      "         [0.2273, 0.2261, 0.2214]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0208, 0.0273],\n",
      "         [0.0123, 0.0225, 0.0241],\n",
      "         [0.0125, 0.0160, 0.0182],\n",
      "         ...,\n",
      "         [0.0106, 0.0113, 0.0094],\n",
      "         [0.0064, 0.0089, 0.0059],\n",
      "         [0.0040, 0.0076, 0.0041]],\n",
      "\n",
      "        [[0.0150, 0.0149, 0.0265],\n",
      "         [0.0136, 0.0189, 0.0179],\n",
      "         [0.0138, 0.0152, 0.0163],\n",
      "         ...,\n",
      "         [0.0116, 0.0140, 0.0087],\n",
      "         [0.0134, 0.0139, 0.0091],\n",
      "         [0.0072, 0.0155, 0.0071]],\n",
      "\n",
      "        [[0.0152, 0.0144, 0.0272],\n",
      "         [0.0109, 0.0140, 0.0157],\n",
      "         [0.0120, 0.0147, 0.0134],\n",
      "         ...,\n",
      "         [0.0227, 0.0288, 0.0075],\n",
      "         [0.0261, 0.0274, 0.0092],\n",
      "         [0.0118, 0.0318, 0.0067]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.array LoadImage.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "# LOAD WITH STANDARD IMAGE LOADER TO COMPARE\n",
    "data_list = [train_files[0]['image_1'], train_files[0]['image_2']]\n",
    "\n",
    "load_file = Compose([LoadImage(ensure_channel_first=False)])\n",
    "output_original = load_file(data_list)\n",
    "metatensor_1 = output_original[0][0]\n",
    "metatensor_2 = output_original[1][0]\n",
    "print(metatensor_1.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[[0.3549, 0.3380, 0.3352],\n",
      "         [0.3519, 0.3691, 0.3125],\n",
      "         [0.3634, 0.3661, 0.3520],\n",
      "         ...,\n",
      "         [0.2452, 0.2340, 0.1959],\n",
      "         [0.2745, 0.2437, 0.2493],\n",
      "         [0.3192, 0.2761, 0.2805]],\n",
      "\n",
      "        [[0.3495, 0.3437, 0.3300],\n",
      "         [0.3317, 0.3535, 0.3084],\n",
      "         [0.3516, 0.3400, 0.3551],\n",
      "         ...,\n",
      "         [0.2688, 0.2568, 0.1992],\n",
      "         [0.2424, 0.2224, 0.1931],\n",
      "         [0.2454, 0.2236, 0.2136]],\n",
      "\n",
      "        [[0.3308, 0.3461, 0.3214],\n",
      "         [0.3149, 0.3304, 0.3110],\n",
      "         [0.3439, 0.3199, 0.3542],\n",
      "         ...,\n",
      "         [0.3304, 0.3056, 0.2455],\n",
      "         [0.2756, 0.2674, 0.2069],\n",
      "         [0.2273, 0.2261, 0.2214]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0208, 0.0273],\n",
      "         [0.0123, 0.0225, 0.0241],\n",
      "         [0.0125, 0.0160, 0.0182],\n",
      "         ...,\n",
      "         [0.0106, 0.0113, 0.0094],\n",
      "         [0.0064, 0.0089, 0.0059],\n",
      "         [0.0040, 0.0076, 0.0041]],\n",
      "\n",
      "        [[0.0150, 0.0149, 0.0265],\n",
      "         [0.0136, 0.0189, 0.0179],\n",
      "         [0.0138, 0.0152, 0.0163],\n",
      "         ...,\n",
      "         [0.0116, 0.0140, 0.0087],\n",
      "         [0.0134, 0.0139, 0.0091],\n",
      "         [0.0072, 0.0155, 0.0071]],\n",
      "\n",
      "        [[0.0152, 0.0144, 0.0272],\n",
      "         [0.0109, 0.0140, 0.0157],\n",
      "         [0.0120, 0.0147, 0.0134],\n",
      "         ...,\n",
      "         [0.0227, 0.0288, 0.0075],\n",
      "         [0.0261, 0.0274, 0.0092],\n",
      "         [0.0118, 0.0318, 0.0067]]])\n"
     ]
    }
   ],
   "source": [
    "# TEST NEW CUSTOM LOADER\n",
    "\n",
    "itk_image_1 = itk.imread(train_files[0]['image_1']).astype(itk.F)\n",
    "itk_image_2 = itk.imread(train_files[0]['image_2']).astype(itk.F)\n",
    "data_list = [itk_image_1, itk_image_2]\n",
    "\n",
    "load_obj = Compose([LoadITKObject(ensure_channel_first=False)])\n",
    "output_original = load_obj(data_list)\n",
    "metatensor_1 = output_original[0][0]\n",
    "metatensor_2 = output_original[1][0]\n",
    "print(metatensor_1.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x2a741cd90>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mdata, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(\u001b[39m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m transform(data, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/utility/dictionary.py:282\u001b[0m, in \u001b[0;36mEnsureChannelFirstd.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data: Mapping[Hashable, torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[Hashable, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 282\u001b[0m     d \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(data)\n\u001b[1;32m    283\u001b[0m     \u001b[39mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_iterator(d, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_keys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_key_postfix):\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 192; 2 is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inference_transform \u001b[39m=\u001b[39m Compose([LoadITKObject(), EnsureChannelFirstd(keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage_1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mimage_2\u001b[39m\u001b[39m\"\u001b[39m])])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_original \u001b[39m=\u001b[39m inference_transform(data_list)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/compose.py:322\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_, start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, threading\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, lazy: \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 322\u001b[0m     result \u001b[39m=\u001b[39m execute_compose(\n\u001b[1;32m    323\u001b[0m         input_,\n\u001b[1;32m    324\u001b[0m         transforms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms,\n\u001b[1;32m    325\u001b[0m         start\u001b[39m=\u001b[39;49mstart,\n\u001b[1;32m    326\u001b[0m         end\u001b[39m=\u001b[39;49mend,\n\u001b[1;32m    327\u001b[0m         map_items\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_items,\n\u001b[1;32m    328\u001b[0m         unpack_items\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpack_items,\n\u001b[1;32m    329\u001b[0m         lazy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy,\n\u001b[1;32m    330\u001b[0m         overrides\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moverrides,\n\u001b[1;32m    331\u001b[0m         threading\u001b[39m=\u001b[39;49mthreading,\n\u001b[1;32m    332\u001b[0m         log_stats\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_stats,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[39m=\u001b[39m deepcopy(_transform) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[39melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[39m=\u001b[39m apply_transform(\n\u001b[1;32m    112\u001b[0m         _transform, data, map_items, unpack_items, lazy\u001b[39m=\u001b[39;49mlazy, overrides\u001b[39m=\u001b[39;49moverrides, log_stats\u001b[39m=\u001b[39;49mlog_stats\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m data \u001b[39m=\u001b[39m apply_pending_transforms(data, \u001b[39mNone\u001b[39;00m, overrides, logger_name\u001b[39m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[39m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapplying transform \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x2a741cd90>"
     ]
    }
   ],
   "source": [
    "inference_transform = Compose([LoadITKObject(), EnsureChannelFirstd(keys=[\"image_1\", \"image_2\"])])\n",
    "output_original = inference_transform(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 192, 192)\n",
      "Label shape: (1, 3, 192, 192)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da17bf04b8bb499b88085a25f90182b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5331b1ccf0864ed7a62d3dc1585932fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c571c8ecc4e94bf6f703c0a2a150a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 48, 48)\n",
      "Label shape: (1, 3, 48, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9349e72838473da8ef0bacae1c7e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a87f5b150749cdabe0bd7c7a6bbd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4767a7700b8447b9bb249110b509028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 48, 48)\n",
      "Label shape: (1, 3, 48, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e616f08b4c9b4fbf83569768d0148ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4619c5bcf5ef48c78c4e5fc09e6284e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f055887b1fec488a825286416f2317e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 48, 48)\n",
      "Label shape: (1, 3, 48, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a72660623604c1eaab9ca346d6a9e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925306fcd7264bffa76181fe0cb7af4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72455b988289485f9ff51ec94c2123b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 48, 48)\n",
      "Label shape: (1, 3, 48, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f807f0fa86546169d6a4a8beb85342b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d896f7f67554be88b80db2657b103fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddc122d2cee4f43a241fa94df2d897a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 48, 48)\n",
      "Label shape: (1, 3, 48, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ca5c0b6f9f4b5b8ecf1839c89e9663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7934f8a3025b4fafafb0f3f4210f3895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4af7834bf248bf8ede5e75d11b30c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define preprocessing transforms\n",
    "# Load images\n",
    "if param.in_channels==2:\n",
    "    # Two channels input\n",
    "    transform_array = [\n",
    "        LoadImaged(keys=[\"image_1\", \"image_2\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image_1\", \"image_2\", \"label\"]), # Mariana: AddChanneld(keys=[\"image\", \"label\"]) deprecated, use EnsureChannelFirst instead\n",
    "        ConcatItemsd(keys=[\"image_1\", \"image_2\"], name=\"image\")\n",
    "    ]\n",
    "else:\n",
    "    # One channel input\n",
    "    transform_array = [            \n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim='no_channel'), # Mariana: AddChanneld(keys=[\"image\", \"label\"]) deprecated, use EnsureChannelFirst instead\n",
    "    ]\n",
    "    \n",
    "# Intensity adjustment\n",
    "if (param.input_type == 'R') or (param.input_type == 'I'):\n",
    "    transform_array.append(AdjustContrastd(keys=[\"image\"], gamma=2.5))\n",
    "    transform_array.append(ScaleIntensityd(keys=[\"image\", \"label\"], minv=0, maxv=1, channel_wise=True))\n",
    "\n",
    "# Spatial adjustments\n",
    "transform_array.append(Orientationd(keys=[\"image\", \"label\"], axcodes=param.axcodes))\n",
    "transform_array.append(Spacingd(keys=[\"image\", \"label\"], pixdim=param.pixel_dim, mode=(\"bilinear\", \"nearest\")))\n",
    "\n",
    "# Plot original\n",
    "loadTest = Compose(transform_array)\n",
    "output_original = loadTest(train_files)\n",
    "\n",
    "for i in range(len(output_original)):\n",
    "    if len(output_original)==1:\n",
    "        output_dict = output_original[0]\n",
    "    else:\n",
    "        output_dict = output_original[0][i]\n",
    "    # output_dict = output[0]\n",
    "    image = output_dict['image']\n",
    "    label = output_dict['label']\n",
    "    # image = output_dict[0]['image']\n",
    "    # label = output_dict[0]['label']\n",
    "    image_array = np.array(image)\n",
    "    label_array = np.array(label)\n",
    "    print('Image shape: '+ str(image_array.shape))\n",
    "    print('Label shape: '+ str(label_array.shape))\n",
    "    show_array(image_array[0,:,:,:], title='Ch1 Original')\n",
    "    if image_array.shape[0]==2:\n",
    "        show_array(image_array[1,:,:,:], title='Ch2 Original')\n",
    "    show_array(label_array[0,:,:,:], title='Label Original')\n",
    "\n",
    "# Data augmentation\n",
    "transform_array.append(RandZoomd(\n",
    "    keys=['image', 'label'],\n",
    "    prob=0.1,\n",
    "    min_zoom=1.0,\n",
    "    max_zoom=1.3,\n",
    "    mode=['area', 'nearest'],\n",
    "))\n",
    "transform_array.append(RandFlipd(\n",
    "    keys=['image', 'label'],\n",
    "    prob=0.5,\n",
    "    spatial_axis=2,\n",
    "))\n",
    "# Balance background/foreground\n",
    "transform_array.append(RandCropByPosNegLabeld(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    label_key=\"label\",\n",
    "    spatial_size=param.window_size,\n",
    "    pos=5, \n",
    "    neg=1,\n",
    "    num_samples=5,\n",
    "    image_key=\"image\",\n",
    "    image_threshold=0, \n",
    "))\n",
    "\n",
    "transfTest = Compose(transform_array)\n",
    "output = transfTest(train_files)\n",
    "N = len(output[0])\n",
    "for i in range(N):\n",
    "    output_dict = output[0][i]\n",
    "    image = output_dict['image']\n",
    "    label = output_dict['label']\n",
    "    image_array = np.array(image)\n",
    "    label_array = np.array(label)\n",
    "    print('Image shape: '+ str(image_array.shape))\n",
    "    print('Label shape: '+ str(label_array.shape))\n",
    "    show_array(image_array[0,:,:,:], title='Ch1 '+ str(i+1))\n",
    "    if image_array.shape[0]==2:\n",
    "        show_array(image_array[1,:,:,:], title='Ch2 '+ str(i+1))\n",
    "    show_array(label_array[0,:,:,:], title='Label '+ str(i+1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code to be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from monai.networks.nets import UNet\n",
    "\n",
    "# # Define the UNet architecture\n",
    "# model_unet = UNet(\n",
    "#     spatial_dims=3,\n",
    "#     in_channels=1,\n",
    "#     out_channels=2,\n",
    "#     channels=[16, 32, 64, 128],\n",
    "#     strides=[(1, 2, 2), (1, 2, 2), (1, 1, 1)],\n",
    "#     num_res_units=2,\n",
    "# )\n",
    "\n",
    "# # Create an example input tensor\n",
    "# input_tensor = torch.randn(1, 1, 3, 192, 192)\n",
    "\n",
    "# # Pass the input tensor through the UNet model\n",
    "# output_tensor = model_unet(input_tensor)\n",
    "\n",
    "# # Print the size of the output tensor\n",
    "# print(output_tensor.size())  # Output: torch.Size([1, 2, 3, 192, 192])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
