{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for testing MONAI transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "path = os.path.dirname(current_directory)\n",
    "sys.path.append(path)\n",
    "from Utils import *\n",
    "\n",
    "%matplotlib widget\n",
    "from ipywidgets import interact, interactive, widgets\n",
    "from matplotlib.patches import Rectangle, Circle, Arrow\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Any, Mapping, Hashable\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.utils import first\n",
    "from monai.config import KeysCollection\n",
    "from monai.data import Dataset, ArrayDataset, create_test_image_3d, DataLoader\n",
    "from monai.transforms import (\n",
    "    AdjustContrastd,\n",
    "    Transform,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    Orientation,\n",
    "    ConcatItemsd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureChannelFirst,\n",
    "    ToTensord,\n",
    "    Spacingd,\n",
    "    ScaleIntensityd,\n",
    "    CropForegroundd,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param():\n",
    "    def __init__(self, data_dir, pixel_dim, window_size, orientation, in_channels, out_channels, input_type, num_samples):\n",
    "        self.data_dir = data_dir\n",
    "        self.pixel_dim = pixel_dim\n",
    "        self.window_size = window_size\n",
    "        self.axcodes = orientation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_type = input_type\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "\n",
    "def generateLabeledFileList(param, prefix):\n",
    "    print('Reading labeled images from: ' + param.data_dir)\n",
    "    images_m = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_M.nii.gz\")))\n",
    "    images_p = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_P.nii.gz\")))\n",
    "    images_r = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_R.nii.gz\")))\n",
    "    images_i = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_images\", \"*_I.nii.gz\")))\n",
    "    labels = sorted(glob.glob(os.path.join(param.data_dir, prefix + \"_labels\", \"*_both_label.nii.gz\")))\n",
    "    \n",
    "    # Use two types of images combined\n",
    "    if param.in_channels==2:\n",
    "        # Use real and imaginary images\n",
    "        if param.input_type=='R' or param.input_type=='I':\n",
    "            print('Use real/imaginary images')\n",
    "            data_dicts = [\n",
    "                {\"image_1\": image_r_name, \"image_2\": image_i_name, \"label\":label_name}\n",
    "                for image_r_name, image_i_name, label_name in zip(images_r, images_i, labels)\n",
    "            ]\n",
    "        # Use magnitude and phase images\n",
    "        else:\n",
    "            print('Use magnitude/phase images')\n",
    "            data_dicts = [\n",
    "                {\"image_1\": image_m_name, \"image_2\": image_p_name, \"label\":label_name}\n",
    "                for image_m_name, image_p_name, label_name in zip(images_m, images_p, labels)\n",
    "            ]\n",
    "    # Use only one type of image        \n",
    "    else:\n",
    "        # Use real images\n",
    "        if param.input_type=='R':\n",
    "            print('Use real images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_r, labels)\n",
    "            ]\n",
    "        # Use imaginary images\n",
    "        elif param.input_type=='I':\n",
    "            print('Use imaginary images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_i, labels)\n",
    "            ]\n",
    "        # Use phase images\n",
    "        elif param.input_type=='P':\n",
    "            print('Use phase images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_p, labels)\n",
    "            ]\n",
    "        # Use magnitude images\n",
    "        else:\n",
    "            print('Use magnitude images')\n",
    "            data_dicts = [\n",
    "                {\"image\": image_name, \"label\": label_name}\n",
    "                for image_name, label_name in zip(images_m, labels)\n",
    "            ]\n",
    "    return data_dicts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dictionary\n",
      "Reading labeled images from: /Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook\n",
      "Use magnitude/phase images\n"
     ]
    }
   ],
   "source": [
    "# Build param (info from config.ini)\n",
    "data_dir = os.getcwd()\n",
    "pixel_dim = (3.6, 1.171875, 1.171875)\n",
    "window_size = (3, 160, 160)\n",
    "orientation = 'PIL'\n",
    "in_channels = 2\n",
    "out_channels = 2\n",
    "input_type = 'M'\n",
    "num_samples = 2\n",
    "param = Param(data_dir, pixel_dim, window_size, orientation, in_channels, out_channels, input_type, num_samples)\n",
    "\n",
    "# Create dictionary\n",
    "print('Create dictionary')\n",
    "train_files = generateLabeledFileList(param, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (2, 3, 192, 192)\n",
      "Label shape: (1, 3, 192, 192)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b80eb88fa447a4b34db508c090e147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a9b7b3119a41b2bcc4c5614a1e018a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92f33ab461640f6bf05ec6988d39801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='z', max=2), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.spatial.dictionary.RandZoomd object at 0x2a21022b0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mdata, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(\u001b[39m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m transform(data, lazy\u001b[39m=\u001b[39;49mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/spatial/dictionary.py:2093\u001b[0m, in \u001b[0;36mRandZoomd.__call__\u001b[0;34m(self, data, lazy)\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[39m    data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[39m    a dictionary containing the transformed data, as well as any other data present in the dictionary\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2093\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(data)\n\u001b[1;32m   2094\u001b[0m first_key: Hashable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_key(d)\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 7; 2 is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Balance background/foreground\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# transform_array.append(RandCropByPosNegLabeld(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m#     keys=[\"image\", \"label\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m#     image_threshold=0, #0.05\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# ))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m transfTest \u001b[39m=\u001b[39m Compose(transform_array)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m output \u001b[39m=\u001b[39m transfTest(train_files)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(param\u001b[39m.\u001b[39mnum_samples):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39m# if len(output)==1:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39m#     output_dict = output[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pl771/Devel/MRINeedleSegmentation/TestingNotebook/TestPreprocessing.ipynb#X15sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     output_dict \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][i]\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/compose.py:322\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_, start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, threading\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, lazy: \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 322\u001b[0m     result \u001b[39m=\u001b[39m execute_compose(\n\u001b[1;32m    323\u001b[0m         input_,\n\u001b[1;32m    324\u001b[0m         transforms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms,\n\u001b[1;32m    325\u001b[0m         start\u001b[39m=\u001b[39;49mstart,\n\u001b[1;32m    326\u001b[0m         end\u001b[39m=\u001b[39;49mend,\n\u001b[1;32m    327\u001b[0m         map_items\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_items,\n\u001b[1;32m    328\u001b[0m         unpack_items\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpack_items,\n\u001b[1;32m    329\u001b[0m         lazy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy,\n\u001b[1;32m    330\u001b[0m         overrides\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moverrides,\n\u001b[1;32m    331\u001b[0m         threading\u001b[39m=\u001b[39;49mthreading,\n\u001b[1;32m    332\u001b[0m         log_stats\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_stats,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[39m=\u001b[39m deepcopy(_transform) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[39melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[39m=\u001b[39m apply_transform(\n\u001b[1;32m    112\u001b[0m         _transform, data, map_items, unpack_items, lazy\u001b[39m=\u001b[39;49mlazy, overrides\u001b[39m=\u001b[39;49moverrides, log_stats\u001b[39m=\u001b[39;49mlog_stats\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m data \u001b[39m=\u001b[39m apply_pending_transforms(data, \u001b[39mNone\u001b[39;00m, overrides, logger_name\u001b[39m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.9/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[39m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapplying transform \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.spatial.dictionary.RandZoomd object at 0x2a21022b0>"
     ]
    }
   ],
   "source": [
    "# Define preprocessing transforms\n",
    "# Load images\n",
    "if param.in_channels==2:\n",
    "    # Two channels input\n",
    "    transform_array = [\n",
    "        LoadImaged(keys=[\"image_1\", \"image_2\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image_1\", \"image_2\", \"label\"]), # Mariana: AddChanneld(keys=[\"image\", \"label\"]) deprecated, use EnsureChannelFirst instead\n",
    "        ConcatItemsd(keys=[\"image_1\", \"image_2\"], name=\"image\")\n",
    "    ]\n",
    "else:\n",
    "    # One channel input\n",
    "    transform_array = [            \n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim='no_channel'), # Mariana: AddChanneld(keys=[\"image\", \"label\"]) deprecated, use EnsureChannelFirst instead\n",
    "    ]\n",
    "    \n",
    "# Intensity adjustment\n",
    "if (param.input_type == 'R') or (param.input_type == 'I'):\n",
    "    transform_array.append(AdjustContrastd(keys=[\"image\"], gamma=2.5))\n",
    "transform_array.append(ScaleIntensityd(keys=[\"image\", \"label\"], minv=0, maxv=1, channel_wise=True))\n",
    "\n",
    "# Spatial adjustments\n",
    "transform_array.append(Orientationd(keys=[\"image\", \"label\"], axcodes=param.axcodes))\n",
    "transform_array.append(Spacingd(keys=[\"image\", \"label\"], pixdim=param.pixel_dim, mode=(\"bilinear\", \"nearest\")))\n",
    "\n",
    "# Plot original\n",
    "loadTest = Compose(transform_array)\n",
    "output_original = loadTest(train_files)\n",
    "\n",
    "for i in range(len(output_original)):\n",
    "    if len(output_original)==1:\n",
    "        output_dict = output_original[0]\n",
    "    else:\n",
    "        output_dict = output_original[0][i]\n",
    "    # output_dict = output[0]\n",
    "    image = output_dict['image']\n",
    "    label = output_dict['label']\n",
    "    # image = output_dict[0]['image']\n",
    "    # label = output_dict[0]['label']\n",
    "    image_array = np.array(image)\n",
    "    label_array = np.array(label)\n",
    "    print('Image shape: '+ str(image_array.shape))\n",
    "    print('Label shape: '+ str(label_array.shape))\n",
    "    show_array(image_array[0,:,:,:], title='Ch1 Original')\n",
    "    if image_array.shape[0]==2:\n",
    "        show_array(image_array[1,:,:,:], title='Ch2 Original')\n",
    "    show_array(label_array[0,:,:,:], title='Label Original')\n",
    "\n",
    "# Data augmentation\n",
    "transform_array.append(RandZoomd(\n",
    "    keys=['image', 'label'],\n",
    "    prob=0.5,\n",
    "    min_zoom=1.1,\n",
    "    max_zoom=1.5,\n",
    "    mode=['area', 'nearest'],\n",
    "))\n",
    "transform_array.append(RandFlipd(\n",
    "    keys=['image', 'label'],\n",
    "    prob=0.5,\n",
    "    spatial_axis=2,\n",
    "))\n",
    "# Balance background/foreground\n",
    "# transform_array.append(RandCropByPosNegLabeld(\n",
    "#     keys=[\"image\", \"label\"],\n",
    "#     label_key=\"label\",\n",
    "#     spatial_size=param.window_size,\n",
    "#     pos=1, #0.8\n",
    "#     neg=1, #0.2\n",
    "#     num_samples=2,\n",
    "#     image_key=\"image\",\n",
    "#     image_threshold=0, #0.05\n",
    "# ))\n",
    "transform_array.append(RandCropByLabelClassesd(\n",
    "    keys=[\"image\", \"label\"], \n",
    "    label_key=\"label\", \n",
    "    spatial_size=param.window_size, \n",
    "    ratios=[1,3], \n",
    "    num_classes=2,\n",
    "    num_samples=param.num_samples, \n",
    "    image_key=\"image\", \n",
    "    image_threshold=0,\n",
    "))\n",
    "\n",
    "\n",
    "transfTest = Compose(transform_array)\n",
    "output = transfTest(train_files)\n",
    "\n",
    "for i in range(param.num_samples):\n",
    "    # if len(output)==1:\n",
    "    #     output_dict = output[0]\n",
    "    # else:\n",
    "    output_dict = output[0][i]\n",
    "    image = output_dict['image']\n",
    "    label = output_dict['label']\n",
    "    image_array = np.array(image)\n",
    "    label_array = np.array(label)\n",
    "    print('Image shape: '+ str(image_array.shape))\n",
    "    print('Label shape: '+ str(label_array.shape))\n",
    "    show_array(image_array[0,:,:,:], title='Ch1 '+ str(i))\n",
    "    if image_array.shape[0]==2:\n",
    "        show_array(image_array[1,:,:,:], title='Ch2 '+ str(i))\n",
    "    show_array(label_array[0,:,:,:], title='Label '+ str(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code to be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from monai.networks.nets import UNet\n",
    "\n",
    "# # Define the UNet architecture\n",
    "# model_unet = UNet(\n",
    "#     spatial_dims=3,\n",
    "#     in_channels=1,\n",
    "#     out_channels=2,\n",
    "#     channels=[16, 32, 64, 128],\n",
    "#     strides=[(1, 2, 2), (1, 2, 2), (1, 1, 1)],\n",
    "#     num_res_units=2,\n",
    "# )\n",
    "\n",
    "# # Create an example input tensor\n",
    "# input_tensor = torch.randn(1, 1, 3, 192, 192)\n",
    "\n",
    "# # Pass the input tensor through the UNet model\n",
    "# output_tensor = model_unet(input_tensor)\n",
    "\n",
    "# # Print the size of the output tensor\n",
    "# print(output_tensor.size())  # Output: torch.Size([1, 2, 3, 192, 192])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
